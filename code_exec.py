import os
import sys
import uuid
import tempfile
import importlib.util
import pandas as pd
import matplotlib.pyplot as plt
from docx import Document
from io import StringIO
import sqlite3
import json

class ValidationError(Exception):
    """Custom exception for validation errors."""
    pass

def docx2tabular(docx_path):
    """Convert a DOCX table to a list of rows"""
    doc = Document(docx_path)
    if not doc.tables:
        return None
    table = doc.tables[0]
    return [[cell.text for cell in row.cells] for row in table.rows]


def save_file(upload_file):
    """Save an uploaded file and return its dataframe, text, and metadata"""
    # Create uploads directory if it doesn't exist
    base_dir = 'uploaded_files'
    os.makedirs(base_dir, exist_ok=True)
    
    # Generate a unique path for the file
    file_id = str(uuid.uuid4())[:8]
    local_path = f'{base_dir}/{file_id}-{upload_file.name}'
    
    # Process based on file type
    if upload_file.name.endswith('.csv'):
        df = pd.read_csv(upload_file, encoding="utf-8", on_bad_lines='skip')
        df.to_csv(local_path, index=False)
        query_tabular = upload_file.getvalue().decode("utf-8")

    elif upload_file.name.endswith(('.xlsx', '.xls')):
        df = pd.ExcelFile(upload_file).parse()
        csv_path = local_path.replace(".xlsx", ".csv").replace(".xls", ".csv")
        df.to_csv(csv_path, index=False)
        local_path = csv_path
        with open(local_path, 'r', encoding='utf-8') as fp:
            query_tabular = fp.read()

    elif upload_file.name.endswith('.docx'):
        with open(local_path, "wb") as fpdf:
            fpdf.write(upload_file.read())
        tabular_rows = docx2tabular(local_path)
        if not tabular_rows:
            raise ValueError("No table found in the DOCX file")
        df = pd.DataFrame(tabular_rows[1:], columns=tabular_rows[0])
        query_tabular = '\n'.join(','.join(str(cell) for cell in row) for row in tabular_rows)
    else:
        raise ValueError(f"Unsupported file type: {upload_file.name}")

    file_detail = {
        'name': upload_file.name,
        'local_path': local_path,
        'description': ''
    }

    return df, query_tabular, file_detail


def create_code_file(code_content, query, is_double=False):
    """Create a permanent Python file with the provided code content"""
    # Create directory if it doesn't exist
    code_dir = 'generated_code'
    os.makedirs(code_dir, exist_ok=True)
    
    # Create a sanitized filename from the query
    timestamp = uuid.uuid4().hex[:8]
    sanitized_query = "".join(c if c.isalnum() else "_" for c in query[:30])
    
    if is_double:
        filename = f"{code_dir}/dual_{sanitized_query}_{timestamp}.py"
        function_def = """def analyze_data(df1, df2):
    """
    else:
        filename = f"{code_dir}/single_{sanitized_query}_{timestamp}.py"
        function_def = """def analyze_data(df):
    """
    
    # Create a template with the necessary imports
    template = """# Generated by TableLLM on {date}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

{code_content}

"""
    
    # Fill in the template
    full_code = template.format(
        date=pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
        code_content=code_content.strip()
    )
    
    # Write to permanent file
    with open(filename, 'w') as f:
        f.write(full_code)
    
    return filename

def validation_handling(source_df, target_df, result, target_sap_fields):
    error_msg = ""
    
    # Check 1: Length validation - target_df should never be smaller than result
    if len(target_df) < len(result) and len(target_df) != 0:
        error_msg += f"Error: Target dataframe has fewer rows than result. Target: {len(target_df)}, Result: {len(result)}\n"
    
    # Get non-null columns in both dataframes
    target_not_null_columns = set(target_df.columns[target_df.notna().any()].tolist())
    result_not_null_columns = set(result.columns[result.notna().any()].tolist())
    
    # For debugging purposes
    print("Target non-null columns:", target_not_null_columns)
    print("#########################################")
    print("Result non-null columns:", result_not_null_columns)
    print("#########################################")
    
    # Check if the target_sap_field should be included
    expected_not_null_columns = target_not_null_columns.copy()
    if target_sap_fields in target_df.columns:
        # If the target_sap_field has non-null values or isn't already in the not-null columns,
        # we add it to our expected non-null columns
        if target_sap_fields not in target_not_null_columns or target_df[target_sap_fields].notna().any():
            expected_not_null_columns.add(target_sap_fields)
    
    # Get key columns from session if available
    key_columns = []
    try:
        # This assumes the session_id is available in the current context
        # If not, you might need to modify the execute_code function to pass it
        import os
        import json
        session_dirs = [d for d in os.listdir("sessions") if os.path.isdir(os.path.join("sessions", d))]
        if session_dirs:
            # Use most recently modified session directory
            latest_session = max(session_dirs, key=lambda d: os.path.getmtime(os.path.join("sessions", d)))
            key_mapping_path = os.path.join("sessions", latest_session, "key_mapping.json")
            if os.path.exists(key_mapping_path):
                with open(key_mapping_path, "r") as f:
                    key_mappings = json.load(f)
                    key_columns = [mapping.get("target_col") for mapping in key_mappings if mapping.get("target_col")]
    except Exception as e:
        print(f"Warning: Could not retrieve key columns: {e}")
    
    # Key column validation - ensure key columns are in the result
    for key_col in key_columns:
        if key_col not in result.columns:
            error_msg += f"Error: Key column '{key_col}' missing in result\n"
        elif result[key_col].isna().any():
            error_msg += f"Error: Key column '{key_col}' contains null values\n"
    
    # Now validate that result has the expected non-null columns
    if result_not_null_columns != expected_not_null_columns:
        # Find specific differences
        missing_in_result = expected_not_null_columns - result_not_null_columns
        if missing_in_result:
            error_msg += f"Error: Expected non-null columns missing in result: {missing_in_result}\n"
        
        unexpected_in_result = result_not_null_columns - expected_not_null_columns
        if unexpected_in_result:
            error_msg += f"Error: Unexpected non-null columns in result: {unexpected_in_result}\n"
        
        # Even if the counts match but the columns are different, it's still an error
        if len(result_not_null_columns) == len(expected_not_null_columns):
            error_msg += "Error: Result has the same number of non-null columns as expected, but they are different columns\n"
    else:
        print("Validation passed: Result contains the correct set of non-null columns")
    
    if error_msg:
        raise ValidationError(f"Validation errors:\n{error_msg}")

def execute_code(file_path, source_dfs, target_df, target_sap_fields, session_id=None):
    """Execute a Python file and return the result or detailed error traceback"""
    try:
        # Add the current directory to sys.path to ensure utilities can be imported
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        
        # Import utility modules that might be needed by the generated code
        try:
            import transform_utils
        except ImportError:
            pass
            
        # Import the module
        module_name = os.path.basename(file_path).replace('.py', '')
        spec = importlib.util.spec_from_file_location(module_name, file_path)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        # Get key mappings if session_id is provided
        key_mappings = []
        if session_id:
            try:
                key_mapping_path = os.path.join("sessions", session_id, "key_mapping.json")
                if os.path.exists(key_mapping_path):
                    with open(key_mapping_path, "r") as f:
                        key_mappings = json.load(f)
                    print(f"Found {len(key_mappings)} key mappings for session {session_id}")
            except Exception as e:
                print(f"Warning: Could not load key mappings: {e}")
        
        # Run the generated code
        result = module.analyze_data(source_dfs, target_df)
        
        # Validate the result
        validation_handling(source_dfs, target_df, result, target_sap_fields)
        
        # Only update the target field if it exists in both result and target_df
        if target_sap_fields in result.columns and target_sap_fields in target_df.columns:
            target_df[target_sap_fields] = result[target_sap_fields]
        elif target_sap_fields in result.columns:
            # If target_sap_fields exists in result but not in target_df, add it
            target_df[target_sap_fields] = result[target_sap_fields]
            
        return target_df
    except Exception as e:
        # Capture the full traceback with detailed information
        import traceback
        error_traceback = {
            "error_type": type(e).__name__,
            "error_message": str(e),
            "traceback": traceback.format_exc(),
            "code_file": file_path
        }
        
        # Return the detailed error information
        return error_traceback
    finally:
        # Restore sys.path
        if os.path.dirname(os.path.abspath(__file__)) in sys.path:
            sys.path.remove(os.path.dirname(os.path.abspath(__file__)))
            

def execute_code_from_resolved_data(file_path, resolved_data):
    """
    Execute code using data from resolved_data
    
    Parameters:
    file_path (str): Path to the Python file to execute
    resolved_data (dict): Resolved data from the planner
    
    Returns:
    The result of the code execution
    """
    try:
        # Connect to the database
        conn = sqlite3.connect('db.sqlite3')
        
        # Extract table names and field names
        print("Resolved data:", resolved_data)
        source_table = resolved_data['source_table_name']
        target_table = resolved_data['target_table_name']
        source_fields = resolved_data['source_field_names']
        target_fields = resolved_data['target_sap_fields']
        
        # Query the source and target tables with specific fields
        source_df = pd.read_sql_query(f"SELECT * FROM {source_table}", conn)[source_fields]
        target_df = pd.read_sql_query(f"SELECT * FROM {target_table}", conn)[target_fields]
        
        # Import the module
        module_name = os.path.basename(file_path).replace('.py', '')
        spec = importlib.util.spec_from_file_location(module_name, file_path)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        # Execute the code with df1 as source and df2 as target
        result = module.analyze_data(source_df, target_df)
        
        conn.close()
        return result
    except Exception as e:
        error_msg = f"Error executing code with resolved data: {str(e)}"
        print(error_msg)
        return error_msg